# Response
## Properties

| Name | Type | Description | Notes |
|------------ | ------------- | ------------- | -------------|
| **metadata** | **Map** | Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.   Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.  | [default to null] |
| **temperature** | **BigDecimal** | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or &#x60;top_p&#x60; but not both.  | [default to 1] |
| **top\_p** | **BigDecimal** | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both.  | [default to 1] |
| **user** | **String** | A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).  | [optional] [default to null] |
| **service\_tier** | [**ServiceTier**](ServiceTier.md) |  | [optional] [default to null] |
| **previous\_response\_id** | **String** | The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about  [conversation state](/docs/guides/conversation-state).  | [optional] [default to null] |
| **model** | [**ModelIdsResponses**](ModelIdsResponses.md) |  | [default to null] |
| **reasoning** | [**Reasoning**](Reasoning.md) |  | [optional] [default to null] |
| **max\_output\_tokens** | **Integer** | An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).  | [optional] [default to null] |
| **instructions** | **String** | Inserts a system (or developer) message as the first item in the model&#39;s context.  When using along with &#x60;previous_response_id&#x60;, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses.  | [default to null] |
| **text** | [**ResponseProperties_text**](ResponseProperties_text.md) |  | [optional] [default to null] |
| **tools** | [**List**](Tool.md) | An array of tools the model may call while generating a response. You  can specify which tool to use by setting the &#x60;tool_choice&#x60; parameter.  The two categories of tools you can provide the model are:  - **Built-in tools**: Tools that are provided by OpenAI that extend the   model&#39;s capabilities, like [web search](/docs/guides/tools-web-search)   or [file search](/docs/guides/tools-file-search). Learn more about   [built-in tools](/docs/guides/tools). - **Function calls (custom tools)**: Functions that are defined by you,   enabling the model to call your own code. Learn more about   [function calling](/docs/guides/function-calling).  | [default to null] |
| **tool\_choice** | [**ResponseProperties_tool_choice**](ResponseProperties_tool_choice.md) |  | [default to null] |
| **truncation** | **String** | The truncation strategy to use for the model response. - &#x60;auto&#x60;: If the context of this response and previous ones exceeds   the model&#39;s context window size, the model will truncate the    response to fit the context window by dropping input items in the   middle of the conversation.  - &#x60;disabled&#x60; (default): If a model response will exceed the context window    size for a model, the request will fail with a 400 error.  | [optional] [default to disabled] |
| **id** | **String** | Unique identifier for this Response.  | [default to null] |
| **object** | **String** | The object type of this resource - always set to &#x60;response&#x60;.  | [default to null] |
| **status** | **String** | The status of the response generation. One of &#x60;completed&#x60;, &#x60;failed&#x60;,  &#x60;in_progress&#x60;, or &#x60;incomplete&#x60;.  | [optional] [default to null] |
| **created\_at** | **BigDecimal** | Unix timestamp (in seconds) of when this Response was created.  | [default to null] |
| **error** | [**ResponseError**](ResponseError.md) |  | [default to null] |
| **incomplete\_details** | [**Response_allOf_incomplete_details**](Response_allOf_incomplete_details.md) |  | [default to null] |
| **output** | [**List**](OutputItem.md) | An array of content items generated by the model.  - The length and order of items in the &#x60;output&#x60; array is dependent   on the model&#39;s response. - Rather than accessing the first item in the &#x60;output&#x60; array and    assuming it&#39;s an &#x60;assistant&#x60; message with the content generated by   the model, you might consider using the &#x60;output_text&#x60; property where   supported in SDKs.  | [default to null] |
| **output\_text** | **String** | SDK-only convenience property that contains the aggregated text output  from all &#x60;output_text&#x60; items in the &#x60;output&#x60; array, if any are present.  Supported in the Python and JavaScript SDKs.  | [optional] [default to null] |
| **usage** | [**ResponseUsage**](ResponseUsage.md) |  | [optional] [default to null] |
| **parallel\_tool\_calls** | **Boolean** | Whether to allow the model to run tool calls in parallel.  | [default to true] |

[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)

